---
layout: single
title:  "Active SLAM 관련 수학 노트"
categories:
  - laboratory
tag: [active_slam, math, entropy]
---

## 작성 이유

Active SLAM에서 사용되는 수학을 공부하고, 정제된 지식을 틈틈히 보기 위해서이다. 

비용함수를 구성함에 있어서 수학은 active SLAM과 밀접한 관계를 띄는데, 이를 잘 이해하고 추론하기 위해선 직관적으로 바로 이해할 수 있을 정도로 수학에 익숙해야 한다.

그래서 글을 따로따로 포스팅 해야하나 싶었다. 그러나 필요한 수학들을 한 게시물에 모아서 두는편이 좋겠다고 판단하였기에 앞으로 active SLAM을 다루는 과정에서 쓴다고 판단된 수학 개념들은 모두 이곳에 작성할 예정이고 한번 쓰고 끝이 아니라 계속 수정하면서 거대한 문서로 만들 생각이다.

추가하거나 각 시기마다 어떤 내용이 있었는지는 업데이트 내역에 적어두도록 하겠다.

> 작성자: 조형남-마식
> 업데이트 이력
> 2025/02/02: 게시글 탄생
> 2025/02/02: 


## Information Entropy

### Definitions & Overview

정보 엔트로피(Information Entropy)의 정의는 주로 확률 분포에 기초한 여러 측도로 이루어짐.

- **Information Entropy**는 주어진 확률 분포에 대해 예측 불가능성이나 불확실성을 정량화한 값이다.
- **역사(확장예정)**: 클로드 섀넌(Claude Shannon)이 정보 이론을 창시하면서 도입하였으며, 통신 시스템의 효율성과 데이터 압축의 한계를 이해하는 데 중요한 역할을 함.

### Information Measure

- **정의**: 사건이 발생할 때 얻을 수 있는 정보의 양을 수치화한 것.
- 기본 가정: 발생 확률이 낮은 사건일수록 ‘놀라움’이 크며, 이에 따라 더 많은 정보를 제공한다고 볼 수 있다. 즉 역배일수록 얻어지는 의외성에 대한 기대 정보량을 의미한다고 볼 수 있음.
- 보통 정보량 $I(p)$는 확률 $p$에 대해  
  $$
  I(p) = -\log p
  $$
  (로그의 밑은 보통 2 또는 자연상수 $e$)로 정의됨.

### Shannon Entropy

- **정의**: 이산 확률 변수 $X$가 취할 수 있는 값들의 집합 $\{x_1, x_2, \dots, x_n\}$와 각 값에 대한 확률 $p(x_i)$가 주어졌을 때, 섀넌 엔트로피 $H(X)$는  
  $$
  H(X) = -\sum_{i=1}^{n} p(x_i) \log p(x_i)
  $$
  로 정의된다.
- **의미**: 평균적으로 한 번의 사건 발생으로 얻을 수 있는 정보의 양을 나타냄.
- **특징**:
  - 엔트로피는 $p(x_i)$가 균등할 때(최대 불확실성) 최댓값을 가짐.
  - 로그의 밑에 따라 단위가 결정된다. (밑 2일 경우 단위는 비트, 자연로그일 경우 엔트로피 단위는 나트(nat)).

### Rényi Entropy

- **정의**: Shannon Entropy를 일반화한 형태로, 매개변수 $\alpha$를 도입하여  
  $$
  H_\alpha(X) = \frac{1}{1-\alpha} \log \left(\sum_{i=1}^{n} p(x_i)^\alpha \right)
  $$
  (단, $\alpha \geq 0$이고 $\alpha \neq 1$)로 정의된다.
- **특징**:
  - $\alpha \to 1$인 경우, 레니 엔트로피는 섀넌 엔트로피로 수렴함.
  - $\alpha = 0$인 경우에는 **최소 엔트로피 (min-entropy)**, $\alpha = \infty$인 경우에는 **최대 엔트로피 (max-entropy)**와 유사한 개념을 제공한다.
  - 다양한 $\alpha$ 값에 따라 데이터의 **집중 정도나 희귀 사건에 대한 민감도를 조절**할 수 있다.

### Differential Entropy

- **정의**: 연속 확률 분포 $f(x)$에 대해 다음과 같이 정의됨.  
  $$
  h(X) = -\int_{-\infty}^{\infty} f(x) \log f(x) \, dx
  $$
- **특징**:
  - 이산 엔트로피와 달리 미분 엔트로피는 절대적인 정보량의 척도라기보다는 **분포의 ‘모양’과 관련**된 상대적인 척도로 해석됨.
  - 단위와 스케일 변화에 민감하여, 이산 엔트로피와는 다른 성질을 보임.
  - 데이터 압축 및 통신 이론에서 연속 신호의 정보량을 분석할 때 사용됨.

### Kullback-Leibler Divergence

Relative Entropy 라고도 한다.

- **정의**: 두 확률 분포 $P$와 $Q$ 사이의 차이를 측정하는 척도이다. 이산 분포의 경우,  
  $$
  D_{\mathrm{KL}}(P \parallel Q) = \sum_{i} p(x_i) \log \frac{p(x_i)}{q(x_i)}
  $$
  로 정의된다.
- **의미**:
  - $P$와 $Q$가 얼마나 다른지를 비대칭적으로 측정하며, 정보 손실 또는 추가 비용을 나타낸다.
  - $D_{\mathrm{KL}}(P \parallel Q) \geq 0$ (정보 이론의 Gibbs 부등식에 의해)이며, 두 분포가 동일할 때 0이 된다.
- **응용**: 머신러닝, 통계 추정, 베이지안 추론 등에서 모델 간의 차이를 평가하는 데 사용된다.

### Mutual Information

- **정의**: 두 확률 변수 $X$와 $Y$가 공유하는 정보의 양을 측정하는 지표.  
  $$
  I(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
  $$
- **의미**:
  - $I(X; Y)$는 $X$와 $Y$가 서로 얼마나 의존적인지를 나타내며, 두 변수의 정보적 상관 관계를 정량화한다.
  - 상호 정보는 다음과 같이 조건부 엔트로피와의 관계로도 표현됨.  
    $$
    I(X; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)
    $$
- **특징**:
  - 상호 정보는 항상 0 이상의 값을 가지며, 0인 경우 두 변수가 완전히 독립적임을 의미한다.
  - 통신 채널의 용량 분석, 특징 선택, 클러스터링 등 다양한 분야에서 활용된다.

### 정리
**Shannon Entropy**는 이 개념의 기초를 이루고,

**Rényi Entropy**와 **Differential Entropy**는 그 일반화 및 연속 확률 분포에 대한 확장을 나타내며,

**Kullback-Leibler Divergence**와 **Mutual Information**는 엔트로피 개념을 바탕으로 확률 분포 간의 차이나 변수 간의 상관 관계를 정량화하는 도구이다.

